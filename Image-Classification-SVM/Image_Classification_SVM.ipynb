{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification with SVM"
      ],
      "metadata": {
        "id": "WksVyCED73QZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the libraries"
      ],
      "metadata": {
        "id": "KlUVXsqx73TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "qFZ32n2w8CN6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the dataset"
      ],
      "metadata": {
        "id": "0GcWUAnD73WA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "X, y = fetch_openml('CIFAR_10_small', version=1, return_X_y=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWTxkaNN8Fwh",
        "outputId": "39063706-613a-46c7-e37d-548b4ae473c1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data"
      ],
      "metadata": {
        "id": "pxrp_tYy73ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = X / 255.0 # scale pixel values to range of 0 to 1\n",
        "X = np.reshape(X, (X.shape[0], -1)) # flatten images to 1D vectors\n",
        "y = y.astype(np.int) # convert labels to integers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Hj0cQEv8THB",
        "outputId": "3bc6ed63-3454-4ecc-9fbe-3cde16d60769"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2873b2fa6798>:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = y.astype(np.int) # convert labels to integers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split data into train and test sets"
      ],
      "metadata": {
        "id": "3vOCWTwU8eV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 50000\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_test, y_test = X[train_size:], y[train_size:]"
      ],
      "metadata": {
        "id": "6jxMtQfI8f6J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define pipeline for feature extraction and SVM model"
      ],
      "metadata": {
        "id": "m0-ap7dZ8eYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import svm\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('pca', PCA(n_components=100)),\n",
        "    ('svm', svm.SVC(kernel='rbf'))\n",
        "])"
      ],
      "metadata": {
        "id": "IwEQlMq78qF5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define grid of hyperparameters to search over\n"
      ],
      "metadata": {
        "id": "CEoK0n-i8ebi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10],\n",
        "    'svm__gamma': [0.01, 0.1, 1],\n",
        "}"
      ],
      "metadata": {
        "id": "2SrH0mZY8wNZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model using grid search to find optimal hyperparameters"
      ],
      "metadata": {
        "id": "kyTNkkZI80px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "j3gsAIXb80uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model on test set"
      ],
      "metadata": {
        "id": "D33opT2d85Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "precision = metrics.precision_score(y_test, y_pred, average='macro')\n",
        "recall = metrics.recall_score(y_test, y_pred, average='macro')\n",
        "f1_score = metrics.f1_score(y_test, y_pred, average='macro')\n",
        "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "v-ObBzNu85eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result for evaluating"
      ],
      "metadata": {
        "id": "AqPYTUI09Cbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1_score)\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix)"
      ],
      "metadata": {
        "id": "R7PtxuYM9Cjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualising"
      ],
      "metadata": {
        "id": "6wqaIRXF9Kdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(2, 5, figsize=(10, 5))\n",
        "for i in range(2):\n",
        "    for j in range(5):\n",
        "        index = np.random.randint(len(X_test))\n",
        "        image = X_test[index].reshape((32, 32, 3))\n",
        "        label = y_pred[index]\n",
        "        axs[i][j].imshow(image)\n",
        "        axs[i][j].set_title(\"Predicted Label: {}\".format(label))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UJ1kr6yh9Kjw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}